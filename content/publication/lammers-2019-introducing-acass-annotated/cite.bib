@article{lammers2019IntroducingACASSAnnotated,
 abstract = {Others' movements inform us about their current activities as well as their intentions and emotions. Research on the distinct mechanisms underlying action recognition and emotion inferences has been limited due to a lack of suitable comparative stimulus material. Problematic confounds can derive from low-level physical features, (e.g. luminance), as well as from higher-level psychological features (e.g. stimulus difficulty). Here we present a standardized stimulus dataset, which allows to address both action and emotion recognition with identical stimuli. The stimulus set consists of 792 computer animations with a neutral avatar based on full body motion capture protocols. Motion capture was performed on 22 human volunteers, instructed to perform six everyday activities (mopping, sweeping, painting with a roller, painting with a brush, wiping, sanding) in three different moods (angry, happy, sad). Five-second clips of each motion protocol were rendered into AVI-files using two virtual camera perspectives for each clip. In contrast to video stimuli, the computer animations allowed to standardize the physical appearance of the avatarand to control lighting and coloring conditions, thus reducing the stimulus variation to mere movement. To control for low level optical features of the stimuli, we developed and applied a set of MATLAB routines extracting basic physical features of the stimuli, including average background-foreground proportion and frame-by-frame pixel change dynamics. This information was used to identify outliers and to homogenize the stimuli across action and emotion categories. This led to a smaller stimulus subset (n = 83 animations within the 792 clip database) which only contained two different actions (mopping, sweeping) and two different moods (angry, happy). To further homogenize this stimulus subset with regard to psychological criteria we conducted an online observer study (N = 112 participants) to assess the recognition rates for actions and moods, which led to a final sub-selection of 32 clips (8 per category) within the database. The ACASS database and its subsets provide unique opportunities for research applications in social psychology, social neuroscience and applied clinical studies on communication disorders. All 792 AVI-files, selected subsets, MATLAB code, annotations and motion capture data (FBX-files) are available online.},
 author = {Lammers, Sebastian and Bente, Gary and Tepest, Ralf and Jording, Mathis and Roth, Daniel and Vogeley, Kai},
 doi = {10.3389/frobt.2019.00094},
 file = {C\:\\Users\s̆er\\Nextcloud\\Arbeit\\Resources\\Paper\Łammers_et_al_2019_Introducing ACASS.pdf},
 issn = {2296-9144},
 journal = {Frontiers in Robotics and AI},
 keywords = {Body Motion,Experimental paradigms,human interaction,motion capture,Nonverbal Behavior,social cognition,visual stimuli},
 language = {English},
 shorttitle = {Introducing ACASS},
 title = {Introducing ACASS: An Annotated Character Animation Stimulus Set for Controlled (e)Motion Perception Studies},
 volume = {6},
 year = {2019}
}

