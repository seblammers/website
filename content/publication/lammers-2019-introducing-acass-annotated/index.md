---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Introducing ACASS: An Annotated Character Animation Stimulus Set for Controlled
  (e)Motion Perception Studies'
subtitle: ''
summary: ''
authors:
- Sebastian Lammers
- Gary Bente
- Ralf Tepest
- Mathis Jording
- Daniel Roth
- Kai Vogeley
tags:
- '"Body Motion"'
- '"Experimental paradigms"'
- '"human interaction"'
- '"motion capture"'
- '"Nonverbal Behavior"'
- '"social cognition"'
- '"visual stimuli"'
categories: []
date: '2019-01-01'
lastmod: 2020-10-25T13:45:33+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-10-25T12:45:32.292535Z'
publication_types:
- '2'
abstract: Others' movements inform us about their current activities as well as their
  intentions and emotions. Research on the distinct mechanisms underlying action recognition
  and emotion inferences has been limited due to a lack of suitable comparative stimulus
  material. Problematic confounds can derive from low-level physical features, (e.g.
  luminance), as well as from higher-level psychological features (e.g. stimulus difficulty).
  Here we present a standardized stimulus dataset, which allows to address both action
  and emotion recognition with identical stimuli. The stimulus set consists of 792
  computer animations with a neutral avatar based on full body motion capture protocols.
  Motion capture was performed on 22 human volunteers, instructed to perform six everyday
  activities (mopping, sweeping, painting with a roller, painting with a brush, wiping,
  sanding) in three different moods (angry, happy, sad). Five-second clips of each
  motion protocol were rendered into AVI-files using two virtual camera perspectives
  for each clip. In contrast to video stimuli, the computer animations allowed to
  standardize the physical appearance of the avatarand to control lighting and coloring
  conditions, thus reducing the stimulus variation to mere movement. To control for
  low level optical features of the stimuli, we developed and applied a set of MATLAB
  routines extracting basic physical features of the stimuli, including average background-foreground
  proportion and frame-by-frame pixel change dynamics. This information was used to
  identify outliers and to homogenize the stimuli across action and emotion categories.
  This led to a smaller stimulus subset (n = 83 animations within the 792 clip database)
  which only contained two different actions (mopping, sweeping) and two different
  moods (angry, happy). To further homogenize this stimulus subset with regard to
  psychological criteria we conducted an online observer study (N = 112 participants)
  to assess the recognition rates for actions and moods, which led to a final sub-selection
  of 32 clips (8 per category) within the database. The ACASS database and its subsets
  provide unique opportunities for research applications in social psychology, social
  neuroscience and applied clinical studies on communication disorders. All 792 AVI-files,
  selected subsets, MATLAB code, annotations and motion capture data (FBX-files) are
  available online.
publication: '*Frontiers in Robotics and AI*'
doi: 10.3389/frobt.2019.00094
---
